2024-05-16 17:23:02 +0800   74196 execution.flow     INFO     Start executing nodes in thread pool mode.
2024-05-16 17:23:02 +0800   74196 execution.flow     INFO     Start to run 6 nodes with concurrency level 16.
2024-05-16 17:23:02 +0800   74196 execution.flow     INFO     Executing node Summary. node run id: 894fa468-b9e8-4277-a3c1-dbac62410462_Summary_0
2024-05-16 17:23:02 +0800   74196 execution.flow     INFO     Executing node prepare_examples. node run id: 894fa468-b9e8-4277-a3c1-dbac62410462_prepare_examples_0
2024-05-16 17:23:02 +0800   74196 execution.flow     INFO     Node prepare_examples completes.
2024-05-16 17:23:02 +0800   74196 execution.flow     INFO     Executing node chat_history_summary. node run id: 894fa468-b9e8-4277-a3c1-dbac62410462_chat_history_summary_0
2024-05-16 17:23:03 +0800   74196 execution.flow     INFO     Node Summary completes.
2024-05-16 17:23:03 +0800   74196 execution.flow     INFO     Executing node Embedding_inputs. node run id: 894fa468-b9e8-4277-a3c1-dbac62410462_Embedding_inputs_0
2024-05-16 17:23:03 +0800   74196 execution.flow     INFO     Node Embedding_inputs completes.
2024-05-16 17:23:03 +0800   74196 execution.flow     INFO     Executing node seach_from_ai_search. node run id: 894fa468-b9e8-4277-a3c1-dbac62410462_seach_from_ai_search_0
2024-05-16 17:23:04 +0800   74196 execution.flow     INFO     Node chat_history_summary completes.
2024-05-16 17:23:04 +0800   74196 execution.flow     INFO     Node seach_from_ai_search completes.
2024-05-16 17:23:04 +0800   74196 execution.flow     INFO     Executing node chat. node run id: 894fa468-b9e8-4277-a3c1-dbac62410462_chat_0
2024-05-16 17:23:05 +0800   74196 execution.flow     WARNING  Failed to calculate metrics due to exception: Calculating metrics for model chat16k is not supported..
2024-05-16 17:23:05 +0800   74196 execution.flow     WARNING  Output of chat is not json serializable, use str to store it.
2024-05-16 17:23:05 +0800   74196 execution.flow     INFO     Node chat completes.
2024-05-16 17:23:05 +0800   74196 execution.flow     WARNING  Failed to serialize inputs or output for flow run because of cannot pickle 'generator' object.The inputs and output field in api_calls will be None.
2024-05-16 17:23:17 +0800   74196 execution.flow     INFO     Start executing nodes in thread pool mode.
2024-05-16 17:23:17 +0800   74196 execution.flow     INFO     Start to run 6 nodes with concurrency level 16.
2024-05-16 17:23:17 +0800   74196 execution.flow     INFO     Executing node Summary. node run id: 01d4e5b6-baec-428c-9000-3ef92b0b56bb_Summary_0
2024-05-16 17:23:17 +0800   74196 execution.flow     INFO     Executing node prepare_examples. node run id: 01d4e5b6-baec-428c-9000-3ef92b0b56bb_prepare_examples_0
2024-05-16 17:23:17 +0800   74196 execution.flow     INFO     Node prepare_examples completes.
2024-05-16 17:23:17 +0800   74196 execution.flow     INFO     Executing node chat_history_summary. node run id: 01d4e5b6-baec-428c-9000-3ef92b0b56bb_chat_history_summary_0
2024-05-16 17:23:17 +0800   74196 execution.flow     INFO     Node Summary completes.
2024-05-16 17:23:17 +0800   74196 execution.flow     INFO     Executing node Embedding_inputs. node run id: 01d4e5b6-baec-428c-9000-3ef92b0b56bb_Embedding_inputs_0
2024-05-16 17:23:18 +0800   74196 execution.flow     INFO     Node Embedding_inputs completes.
2024-05-16 17:23:18 +0800   74196 execution.flow     INFO     Executing node seach_from_ai_search. node run id: 01d4e5b6-baec-428c-9000-3ef92b0b56bb_seach_from_ai_search_0
2024-05-16 17:23:19 +0800   74196 execution.flow     INFO     Node seach_from_ai_search completes.
2024-05-16 17:23:19 +0800   74196 execution.flow     INFO     Node chat_history_summary completes.
2024-05-16 17:23:19 +0800   74196 execution.flow     INFO     Executing node chat. node run id: 01d4e5b6-baec-428c-9000-3ef92b0b56bb_chat_0
2024-05-16 17:23:20 +0800   74196 execution.flow     WARNING  Failed to calculate metrics due to exception: Calculating metrics for model chat16k is not supported..
2024-05-16 17:23:20 +0800   74196 execution.flow     WARNING  Output of chat is not json serializable, use str to store it.
2024-05-16 17:23:20 +0800   74196 execution.flow     INFO     Node chat completes.
2024-05-16 17:23:20 +0800   74196 execution.flow     WARNING  Failed to serialize inputs or output for flow run because of cannot pickle 'generator' object.The inputs and output field in api_calls will be None.
2024-05-16 17:23:50 +0800   74196 execution.flow     INFO     Start executing nodes in thread pool mode.
2024-05-16 17:23:50 +0800   74196 execution.flow     INFO     Start to run 6 nodes with concurrency level 16.
2024-05-16 17:23:50 +0800   74196 execution.flow     INFO     Executing node Summary. node run id: 17958224-b550-47ab-9f68-80692ba621db_Summary_0
2024-05-16 17:23:50 +0800   74196 execution.flow     INFO     Executing node prepare_examples. node run id: 17958224-b550-47ab-9f68-80692ba621db_prepare_examples_0
2024-05-16 17:23:50 +0800   74196 execution.flow     INFO     Node prepare_examples completes.
2024-05-16 17:23:50 +0800   74196 execution.flow     INFO     Executing node chat_history_summary. node run id: 17958224-b550-47ab-9f68-80692ba621db_chat_history_summary_0
2024-05-16 17:23:51 +0800   74196 execution.flow     INFO     Node Summary completes.
2024-05-16 17:23:51 +0800   74196 execution.flow     INFO     Executing node Embedding_inputs. node run id: 17958224-b550-47ab-9f68-80692ba621db_Embedding_inputs_0
2024-05-16 17:23:51 +0800   74196 execution.flow     INFO     Node Embedding_inputs completes.
2024-05-16 17:23:51 +0800   74196 execution.flow     INFO     Executing node seach_from_ai_search. node run id: 17958224-b550-47ab-9f68-80692ba621db_seach_from_ai_search_0
2024-05-16 17:23:52 +0800   74196 execution.flow     INFO     Node seach_from_ai_search completes.
2024-05-16 17:23:53 +0800   74196 execution.flow     INFO     Node chat_history_summary completes.
2024-05-16 17:23:53 +0800   74196 execution.flow     INFO     Executing node chat. node run id: 17958224-b550-47ab-9f68-80692ba621db_chat_0
2024-05-16 17:23:53 +0800   74196 execution          WARNING  [chat in line 0 (index starts from 0)] stderr> Exception occurs: BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384 tokens. However, you requested 17650 tokens (13554 in the messages, 4096 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}
2024-05-16 17:23:53 +0800   74196 execution          ERROR    Node chat in line 0 failed. Exception: OpenAI API hits BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384 tokens. However, you requested 17650 tokens (13554 in the messages, 4096 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors].
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/promptflow/tools/common.py", line 353, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/promptflow/tools/aoai.py", line 154, in chat
    completion = self._client.chat.completions.create(**params)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/ray.huang/Desktop/promptflow/src/promptflow-tracing/promptflow/tracing/_integrations/_openai_injector.py", line 88, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/Users/ray.huang/Desktop/promptflow/src/promptflow-tracing/promptflow/tracing/_trace.py", line 470, in wrapped
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_utils/_utils.py", line 277, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/resources/chat/completions.py", line 579, in create
    return self._post(
           ^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py", line 1240, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py", line 921, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/openai/_base_client.py", line 1020, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384 tokens. However, you requested 17650 tokens (13554 in the messages, 4096 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/promptflow/_core/flow_execution_context.py", line 90, in invoke_tool
    result = self._invoke_tool_inner(node, f, kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/promptflow/_core/flow_execution_context.py", line 201, in _invoke_tool_inner
    raise e
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/promptflow/_core/flow_execution_context.py", line 182, in _invoke_tool_inner
    return f(**kwargs)
           ^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/promptflow/executor/flow_executor.py", line 1267, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/Users/ray.huang/Desktop/promptflow/src/promptflow-tracing/promptflow/tracing/_trace.py", line 470, in wrapped
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/promptflow/tools/common.py", line 381, in wrapper
    raise WrappedOpenAIError(e)
promptflow.tools.exception.WrappedOpenAIError: OpenAI API hits BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 16384 tokens. However, you requested 17650 tokens (13554 in the messages, 4096 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]
2024-05-16 17:23:53 +0800   74196 execution.flow     ERROR    Flow execution has failed. Cancelling all running nodes: chat.
